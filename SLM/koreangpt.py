# -*- coding: utf-8 -*-
"""koreanGPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H4CTZ7ZPeCMr4XBXA-LE_Cy3b2fCHHWV
"""

# -*- coding: utf-8 -*-
# Original file: https://colab.research.google.com/drive/1qr69a05YyNYkdYvv5dfvo2LB0QLj64EF
# By: wygo (230320)
# Seminar on creating a ChatGPT-replica using GPT fine-tuning, PPO, and RLHF.

# Import necessary libraries
import os
import json
import argparse
from copy import deepcopy
import logging
from typing import Optional, Dict, Sequence

import torch
import torch.nn as nn
from torch.optim import Adam
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    GPT2LMHeadModel,
    pipeline,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    PreTrainedTokenizerFast
)

from datasets import load_dataset

# Configuration Constants
IGNORE_INDEX = -100
DEFAULT_PAD_TOKEN = "[PAD]"
DEFAULT_EOS_TOKEN = "</s>"
DEFAULT_BOS_TOKEN = "</s>"
DEFAULT_UNK_TOKEN = "</s>"
PROMPT_DICT = {
    "prompt_input": (
        "Below is an instruction that describes a task, paired with an input that provides further context.\n"
        "Write a response that appropriately completes the request.\n\n"
        "### Instruction:\n{prompt}\n\n### Input:\n{input}\n\n### Response:"
    ),
    "prompt_no_input": (
        "Below is an instruction that describes a task.\n"
        "Write a response that appropriately completes the request.\n\n"
        "### Instruction:\n{prompt}\n\n### Response:"
    ),
}

# Utility Functions
def safe_save_model_for_hf_trainer(trainer: Trainer, output_dir: str):
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in list(state_dict.items())}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)

def prepare_data_collator(tokenizer):
    class DataCollatorForSupervisedDataset(object):
        def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
            input_ids, labels = tuple([instance[key] for instance in instances] for key in ("input_ids", "labels"))
            input_ids = torch.nn.utils.rnn.pad_sequence(
                input_ids, batch_first=True, padding_value=tokenizer.pad_token_id
            )
            labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)
            return dict(
                input_ids=input_ids,
                labels=labels,
                attention_mask=input_ids.ne(tokenizer.pad_token_id),
            )
    return DataCollatorForSupervisedDataset()

# SFT Dataset
class SFT_dataset(Dataset):
    def __init__(self, data_path: str, tokenizer, verbose=False):
        super(SFT_dataset, self).__init__()
        logging.warning("Loading data...")
        with open(data_path, "r", encoding='utf-8-sig') as json_file:
            list_data_dict = json.load(json_file)
            if verbose:
                print('## data check ##')
                print((list_data_dict[0]))

        prompt_input, prompt_no_input = PROMPT_DICT["prompt_input"], PROMPT_DICT["prompt_no_input"]
        sources = [
            prompt_input.format_map(example) if example.get("input", "") != "" else prompt_no_input.format_map(example)
            for example in list_data_dict
        ]
        targets = [f"{example['completion']}{tokenizer.eos_token}" for example in list_data_dict]

        if verbose:
            idx = 0
            print((sources[idx]))
            print((targets[idx]))
            print("Tokenizing inputs... This may take some time...")

        examples = [s + t for s, t in zip(sources, targets)]
        sources_tokenized = self._tokenize_fn(sources, tokenizer)
        examples_tokenized = self._tokenize_fn(examples, tokenizer)

        input_ids = examples_tokenized["input_ids"]
        labels = deepcopy(input_ids)
        for label, source_len in zip(labels, sources_tokenized["input_ids_lens"]):
            label[:source_len] = IGNORE_INDEX

        self.input_ids = input_ids
        self.labels = labels
        logging.warning("Loading data done!!: %d"%(len(self.labels)))

    def _tokenize_fn(self, strings: Sequence[str], tokenizer) -> Dict:
        tokenized_list = [
            tokenizer(
                text,
                return_tensors="pt",
                padding="longest",
                max_length=tokenizer.model_max_length,
                truncation=True,
            )
            for text in strings
        ]
        input_ids = [tokenized.input_ids[0] for tokenized in tokenized_list]
        input_ids_lens = [tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list]
        return dict(
            input_ids=input_ids,
            input_ids_lens=input_ids_lens,
        )

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, i) -> Dict[str, torch.Tensor]:
        return dict(input_ids=self.input_ids[i], labels=self.labels[i])

# Main function
def main():
    # Ensure required packages are installed
    os.system('pip install colossalai==0.2.7')
    os.system('pip install openai langchain==0.0.113 pandas>=1.4.1')

    # Argument Parsing
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_path_1_SFT', type=str, default='./data_kochatgpt/kochatgpt_1_SFT.jsonl')
    parser.add_argument('--model_name', type=str, default='gpt2', choices=['gpt2', 'bloom', 'opt'])
    parser.add_argument('--max_epochs', type=int, default=2)
    parser.add_argument('--train_batch_size', type=int, default=8)
    parser.add_argument('--output_dir', type=str, default='./output_1_SFT')
    args = parser.parse_args(args=[])

    # Tokenizer and Model Preparation
    tokenizer = AutoTokenizer.from_pretrained("skt/kogpt2-base-v2", padding_side="right", model_max_length=512)
    tokenizer.add_special_tokens(
        {
            "eos_token": DEFAULT_EOS_TOKEN,
            "bos_token": DEFAULT_BOS_TOKEN,
            "unk_token": DEFAULT_UNK_TOKEN,
        }
    )
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(args.model_name)
    model.resize_token_embeddings(len(tokenizer))

    # Prepare Data
    train_dataset = SFT_dataset(data_path=args.data_path_1_SFT, tokenizer=tokenizer)
    data_collator = prepare_data_collator(tokenizer)

    # Training Arguments
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        overwrite_output_dir=True,
        num_train_epochs=args.max_epochs,
        per_device_train_batch_size=args.train_batch_size,
        save_steps=500,
        warmup_steps=5,
        prediction_loss_only=True,
    )

    # Trainer Setup
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=None,
    )

    # Training
    trainer.train()
    trainer.save_state()
    safe_save_model_for_hf_trainer(trainer=trainer, output_dir=args.output_dir)

    # Inference
    generator = pipeline('text-generation', model=args.output_dir, tokenizer=tokenizer)
    generation_args = dict(
        num_beams=4,
        repetition_penalty=2.0,
        no_repeat_ngram_size=4,
        eos_token_id=375,
        max_new_tokens=64,
        do_sample=True,
        top_k=50,
        early_stopping=True
    )

    list_prompt = [
        '불고기용 고기 한우에요?',
        '리처드 닉슨이 43대 부통령직을 수행한 년도는?',
        '시카고 오헤어 국제공항은 어디에 있어',
        '오늘 미세먼지 어때?'
    ]
    list_prompt = [PROMPT_DICT['prompt_no_input'].format_map({'prompt': tmp}) for tmp in list_prompt]

    list_result = generator(list_prompt, **generation_args)
    for prompt, result in zip(list_prompt, list_result):
        print(('#'*70))
        print(('completion: %s'%(result[0]['generated_text'])))

if __name__ == "__main__":
    main()

